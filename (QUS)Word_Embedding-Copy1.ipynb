{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상환경 구성 및 필요한 라이브러리 설치\n",
    "# conda create -n LM\n",
    "# conda install pip\n",
    "# pip install pickle\n",
    "# conda install gensim \n",
    "# conda install nltk\n",
    "# conda install -c anaconda scikit-learn\n",
    "# conda install pytorch==1.0.0 torchvision==0.2.1 cuda100 -c pytorch\n",
    "# conda install matplotlib\n",
    "# conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ziwpOYaCY_g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To ignore deprecated warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from gensim.models import word2vec #  word2vec 관련 모델을 제공해주는 library  \n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews # corpus 를 가져오는 용도 \n",
    "from sklearn.manifold import TSNE # 시각화를 위한 TSNE 사용 \n",
    "from sklearn.metrics import accuracy_score # accuracy score 를 계산하기 위한 metric 라이브러리 불러오기 \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from torch.utils import data # pytorch data class \n",
    "import torch.nn as nn # pytorch neural network 불러오기 \n",
    "import torch.nn.utils.rnn as rnn_utils # rnn utils\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_AiapuJCY_i"
   },
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSqgaJGKCY_j"
   },
   "source": [
    "# Word embedding (CBOW & Skip-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cxp2WjuaCY_j"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "![대체 텍스트](./figures/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNwWp00-CY_k"
   },
   "source": [
    "![대체 텍스트](./figures/w2vec_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHrhWQYlCY_k"
   },
   "outputs": [],
   "source": [
    "# 파라메터값 지정\n",
    "num_features = 300 # 임베딩 벡터 사이즈\n",
    "negative = 10 # negative sampling할 단어 수\n",
    "min_word_count = 10 # 단어빈도수가 10으로 제한\n",
    "window = 5 # context window 사이즈\n",
    "downsampling = 0.75 # 단어의 빈도수가 높은 단어에 대해 빈도수를 낮춤\n",
    "epoch = 5 # epoch 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kHZIbYMCY_m"
   },
   "outputs": [],
   "source": [
    "# preparing data\n",
    "sentences = []\n",
    "pos_data = open(\"./Data/lm_data/pos/train.txt\").readlines()\n",
    "neg_data = open(\"./Data/lm_data/neg/train.txt\").readlines()\n",
    "data_ = pos_data + neg_data\n",
    "for line in data_:\n",
    "    sentences.append(line.strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7HMVxwYCY_n"
   },
   "outputs": [],
   "source": [
    "# skip-gram 모델 학습\n",
    "skip_gram = word2vec.Word2Vec(sentences,\n",
    "                              sg = 1, # skip-gram\n",
    "                              negative=negative,\n",
    "                              size=num_features, \n",
    "                              min_count=min_word_count,\n",
    "                              window=window,\n",
    "                              sample=downsampling,\n",
    "                              iter=epoch)\n",
    "\n",
    "# CBOW 모델 학습\n",
    "CBOW = word2vec.Word2Vec(sentences,\n",
    "                              sg = 0, # CBOW\n",
    "                              negative=negative,\n",
    "                              size=num_features, \n",
    "                              min_count=min_word_count,\n",
    "                              window=window,\n",
    "                              sample=downsampling,\n",
    "                              iter=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UODwOMC2CY_p"
   },
   "source": [
    "##  Training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1iO_RMTCY_q"
   },
   "outputs": [],
   "source": [
    "skip_gram.wv.most_similar(\"man\") # skip-gram 모델을 이용하여 'man'에 가장 유사한 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFxt2BZ2CY_r"
   },
   "outputs": [],
   "source": [
    "CBOW.wv.most_similar(\"man\") # CBOW 모델을 이용하여 'man'에 가장 유사한 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZH1694CsCY_t"
   },
   "outputs": [],
   "source": [
    "vector = skip_gram[\"king\"] - skip_gram[\"man\"] + skip_gram[\"woman\"] # https://word2vec.kr/search/\n",
    "skip_gram.wv.similar_by_vector(vector, topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI9lbWEmCY_v"
   },
   "outputs": [],
   "source": [
    "def render_TSNE(vocab, word_emb):\n",
    "    \"\"\"\n",
    "    TSNE를 이용한 word2vec 시각화 \n",
    "    args:\n",
    "        vocab    - vocab list\n",
    "        word_emb - word embeddings\n",
    "    \"\"\"\n",
    "    tsne = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32) #TSNE 시각화를 위한 initialization\n",
    "    _tsne = tsne.fit_transform(word_emb) # TSNE 시각화 적용\n",
    "    x_coordinate = _tsne[:,0] # x 좌표\n",
    "    y_coordinate = _tsne[:,1] # y 좌표\n",
    "\n",
    "    # scatter plot initialization\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(40, 20)\n",
    "    ax.scatter(x_coordinate, y_coordinate)\n",
    "\n",
    "    for i, word in enumerate(random_vocab):\n",
    "        ax.annotate(word,(x_coordinate[i], y_coordinate[i]), fontsize=30) # 각 scatter들에대해 단어 labeling\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSQOkHhrCY_x"
   },
   "outputs": [],
   "source": [
    "# Skip-gram 시각화(TSNE)\n",
    "vocab = list(skip_gram.wv.vocab) # vocab list 불러오기\n",
    "random_vocab = random.sample(vocab,k=100) #100개의 임의 단어를 랜덤 샘플링\n",
    "word_emb = skip_gram[random_vocab] # 샘플링된 단어에 대해 학습된 임베딩 벡터 불러오기\n",
    "render_TSNE(random_vocab, word_emb) # TSNE 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZSn0D29CY_y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CBOW 시각화(TSNE)\n",
    "vocab = list(CBOW.wv.vocab) # vocab list 불러오기\n",
    "random_vocab = random.sample(vocab,k=100) #100개의 임의 단어를 랜덤 샘플링\n",
    "word_emb = skip_gram[random_vocab] # 샘플링된 단어에 대해 학습된 임베딩 벡터 불러오기\n",
    "render_TSNE(random_vocab, word_emb) # TSNE 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPj2qvM_CY_0"
   },
   "source": [
    "# Chapter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kf27By2TCY_0"
   },
   "source": [
    "# Lanaguage Model (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCap_GcBCY_1"
   },
   "source": [
    "![대체 텍스트](./figures/lm_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIRsKttPCY_1"
   },
   "outputs": [],
   "source": [
    "def build_dict(seqs):\n",
    "    num_skip_sent = 0\n",
    "    word_count = 4\n",
    "    vocab = [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"]\n",
    "    word2id = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
    "    id2word = {0: \"<pad>\", 1: \"<s>\", 2: \"</s>\", 3: \"<unk>\"}\n",
    "    print(\"Building vocab and dict..\")\n",
    "    for line in seqs:\n",
    "        words = line.strip().split(' ') # tokenized by space \n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                word_count += 1 # increment word_count\n",
    "                vocab.append(word) # append new unique word\n",
    "                index = word_count - 1 # word index (consider index 0)\n",
    "                word2id[word] = index # word to index\n",
    "                id2word[index] = word # index to word\n",
    "    \n",
    "    print(\"Number of unique words: %d\" % len(vocab))\n",
    "    print(\"Finised building vocab and dict!\")\n",
    "\n",
    "    return vocab, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSqDjrDFCY_3"
   },
   "outputs": [],
   "source": [
    "# Training corpus를 사용하여 vocab과 dict 구성\n",
    "assert os.path.isfile(\"./Data/lm_data/pos/train.txt\")\n",
    "assert os.path.isfile(\"./Data/lm_data/neg/train.txt\")\n",
    "pos_seqs = open(\"./Data/lm_data/pos/train.txt\").readlines()\n",
    "neg_seqs = open(\"./Data/lm_data/neg/train.txt\").readlines()\n",
    "seqs = pos_seqs + neg_seqs # 긍정 corpus + 부정 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62966\n",
      "62966\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(seqs)):\n",
    "    words = seqs[i].strip().split(' ')\n",
    "    if len(words) >= 0:\n",
    "        count+=1\n",
    "print(count)\n",
    "print(len(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I09r6KQECY_4"
   },
   "outputs": [],
   "source": [
    "# Building vocab or loading existing vocab\n",
    "path = \"./Data/lm_id2word.pkl\"\n",
    "if os.path.isfile(path):\n",
    "    with open(\"./Data/lm_id2word.pkl\", \"rb\") as f:\n",
    "        id2word = pickle.load(f)\n",
    "    with open(\"./Data/lm_word2id.pkl\", \"rb\") as f:\n",
    "        word2id = pickle.load(f)\n",
    "    with open(\"./Data/lm_vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    vocab, word2id, id2word = build_dict(seqs)\n",
    "    pickle.dump(vocab, open(\"./Data/lm_vocab.pkl\", \"wb\" ))\n",
    "    pickle.dump(word2id, open(\"./Data/lm_word2id.pkl\", \"wb\" ))\n",
    "    pickle.dump(id2word, open(\"./Data/lm_id2word.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvyyefNrCY_6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0}\n",
      "{'<s>': 1}\n",
      "{'</s>': 2}\n",
      "{'<unk>': 3}\n",
      "{'films': 4}\n",
      "{'adapted': 5}\n",
      "{'from': 6}\n",
      "{'comic': 7}\n",
      "{'books': 8}\n",
      "{'have': 9}\n"
     ]
    }
   ],
   "source": [
    "# example of first 10 word2id\n",
    "for index, word in enumerate(word2id.keys()):\n",
    "    print({word: word2id[word]})\n",
    "    if index == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWjkgrE4CY_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>'}\n",
      "{1: '<s>'}\n",
      "{2: '</s>'}\n",
      "{3: '<unk>'}\n",
      "{4: 'films'}\n",
      "{5: 'adapted'}\n",
      "{6: 'from'}\n",
      "{7: 'comic'}\n",
      "{8: 'books'}\n",
      "{9: 'have'}\n"
     ]
    }
   ],
   "source": [
    "# example of first 10 id2word\n",
    "for index, id in enumerate(id2word.keys()):\n",
    "    print({id: id2word[id]})\n",
    "    if index == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7V7wOngUCY_9"
   },
   "outputs": [],
   "source": [
    "class LM_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, pos_path, neg_path , word2id):\n",
    "        \n",
    "        #(add+)\n",
    "        self.pos_seqs = open(pos_path).readlines()\n",
    "        self.pos_labels = [0] * len(self.pos_seqs)\n",
    "        self.neg_seqs = open(neg_path).readlines()\n",
    "        self.neg_labels = [1] * len(self.neg_seqs)\n",
    "        self.seqs = self.pos_seqs + self.neg_seqs # concat pos_seqs and neg_seqs\n",
    "        self.labels = self.pos_labels + self.neg_labels\n",
    "        \n",
    "        self.word2id = word2id\n",
    "\n",
    "\n",
    "    def __getitem__(self, index): # 가장 중요한 part Dataset 을 상속한 클래스는 이 부분을 overriding 해줘야함. \n",
    "        \"\"\"Returns one data pair (source and sentiment).\"\"\"\n",
    "        seqs = self.seqs[index]\n",
    "        seqs = self.process(seqs, self.word2id)\n",
    "        sentiment_labels = self.labels[index]\n",
    "        return seqs, sentiment_labels\n",
    "\n",
    "    def __len__(self): # 필수적인 클래스 이 부분을 무조건 overriding 해야함 \n",
    "        return len(self.seqs) # 데이터 갯수 return \n",
    "                    \n",
    "    def process(self, seq, word2id):\n",
    "        sequence = []\n",
    "#         sequence.append(word2id[\"<s>\"])\n",
    "        words = seq.strip().split(' ')\n",
    "#         if len(words) <= 50: # max seq_len = 100\n",
    "        for word in words:\n",
    "            if word in word2id:\n",
    "                sequence.append(word2id[word]) # \n",
    "            else:\n",
    "                sequence.append(3) # replace by <unk> token\n",
    "#         sequence.append(word2id[\"</s>\"])\n",
    "        sequence = torch.Tensor(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKYSwlTICY__"
   },
   "source": [
    "### Batch 를 구성해보겠습니다. \n",
    "Q : 우리의 문장에서 항상 단어의 갯수는 일정한가요?\n",
    "\n",
    "문제점: Tensor로 변환하기 위해서는 추가 처리 필요 . eg. Sentence1 : W1 , W2, W3 ,W4 / Sentence : W1 , W2 ,W3 ->  2*3이 맞나요? 2*4가 맞나요? \n",
    "\n",
    "해결법 : Batch * max_length  Tensor로 변환합니다. eg. 2*4의 경우 2 번째 문장의 마지막에 PAD_token을 처리해줍니다. Index 보통 : 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWcvPpkuCY__"
   },
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, value=0, dim=0):\n",
    "    \"\"\"\n",
    "    pad token으로 채우는 용도 \n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = pad - vec.shape[0]\n",
    "\n",
    "    if len(vec.shape) == 2:\n",
    "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
    "    elif len(vec.shape) == 1:\n",
    "        zeros = torch.ones((pad_size,)) * value\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMvUF-0_CZAB"
   },
   "source": [
    "![대체 텍스트](./figures/sorted.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = lambda x: x*x\n",
    "temp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x_Dhb1sCZAC"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, values=(0, 0), dim=0):\n",
    "    \"\"\"\n",
    "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
    "    args:\n",
    "        batch - list of (tensor, label)\n",
    "    reutrn:\n",
    "        xs - a tensor of all examples in 'batch' after padding\n",
    "        ys - a LongTensor of all labels in batch\n",
    "        ws - a tensor of sequence lengths\n",
    "    \"\"\"\n",
    "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
    "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
    "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
    "    # map(함수, 리스트) -> list\n",
    "    # lambda 인자: 표현식\n",
    "    max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
    "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
    "    batch = [(pad_tensor(x, pad=max_len, dim=dim), label) for (x, label) in batch]\n",
    "\n",
    "    # stack all\n",
    "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
    "    xs = xs[xids].contiguous() # decreasing order로 다시 나열\n",
    "    \n",
    "    labels = [x[1] for x in batch]\n",
    "    labels = [labels[i] for i in xids] # decreasing order로 다시 나열\n",
    "    \n",
    "    return xs.long(), sequence_lengths.int(), torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSrceFNvCZAD"
   },
   "source": [
    "![대체 텍스트](./figures/many_many.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iT7sW1n1CZAE"
   },
   "source": [
    "### Build Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bv9tNwFgCZAE"
   },
   "source": [
    "![대체 텍스트](./figures/sorted.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTWZuD7zCZAE"
   },
   "source": [
    "![대체 텍스트](./figures/GRU.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlxP_z1ICZAF"
   },
   "source": [
    "![대체 텍스트](./figures/GRU_param.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4knJ9ZICZAF"
   },
   "source": [
    "![대체 텍스트](./figures/gru_output2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghmVZC2nCZAF"
   },
   "source": [
    "### Example  pack padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OxvzXygCZAG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [3, 4, 0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UsW91gc9CZAH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1, 3, 2, 4, 3]), batch_sizes=tensor([2, 2, 1]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack_b=torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[3,2])\n",
    "pack_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx0EO9QDCZAI",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [3, 4, 0]]),\n",
       " tensor([3, 2]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_utils.pad_packed_sequence(pack_b,batch_first=True,padding_value=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjZe6D0xCZAK"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_R-mDS4CZAL"
   },
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(LM, self).__init__()\n",
    "        self.vocab_size = vocab_size # get vocabulary size \n",
    "        self.embedding = nn.Embedding(self.vocab_size, config[\"embedding_size\"])  # initialize embedding layer \n",
    "        self.rnn = nn.RNN(input_size=config[\"embedding_size\"], hidden_size=config[\"hidden_size\"], \n",
    "                          dropout=config[\"dropout\"], batch_first=True) # defined rnn layer \n",
    "        self.outputs2vocab = nn.Linear(config[\"hidden_size\"], self.vocab_size) # linear layer to predict the sentiment\n",
    "    def forward(self,input, lengths):\n",
    "        batch_size = input.size(0) # get batch_size\n",
    "        input_embedding = self.embedding(input) # embedding layer  (B,L) -> (B,L ,hidden)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, lengths, batch_first=True) # pack sequence \n",
    "        outputs, _ = self.rnn(packed_input)   # rnn \n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True, padding_value=0.)[0] # packed sequence \n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        outputs = self.outputs2vocab(padded_outputs) \n",
    "        logits = outputs.contiguous().view(-1, self.vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBJLKaE2CZAN"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 10,\n",
    "  \"hidden_size\": 128,\n",
    "  \"dropout\": 0.5,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 256,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PoryeXDiCZAO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = LM_Dataset(\"./Data/lm_data/pos/train.txt\", \"./Data/lm_data/neg/train.txt\", word2id)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "valid_dataset = LM_Dataset(\"./Data/lm_data/pos/valid.txt\", \"./Data/lm_data/neg/valid.txt\", word2id)\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)\n",
    "test_dataset = LM_Dataset(\"./Data/lm_data/pos/test.txt\", \"./Data/lm_data/neg/test.txt\", word2id)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxvDSikNCZAQ"
   },
   "source": [
    "![대체 텍스트](./figures/crossentropy.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsgOhq8-CZAQ"
   },
   "source": [
    "![대체 텍스트](./figures/crossentropy_2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrfu8KvMCZAQ"
   },
   "source": [
    "![대체 텍스트](./figures/p13.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aprjImeOCZAQ"
   },
   "source": [
    "![대체 텍스트](./figures/p14.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqe43xMECZAR"
   },
   "source": [
    "![대체 텍스트](./figures/p17.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVX-Xsl-CZAR"
   },
   "outputs": [],
   "source": [
    "def lm_train(model, optimizer, train_loader, epoch,n_epochs):\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    model.train() # train mode\n",
    "    n_iter = 0\n",
    "    for batch in train_loader:\n",
    "        input, input_lengths, _ = batch \n",
    "        output = model(input[:,:-1].contiguous().to(device),(input_lengths-1).to(device))\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "        target = input[:,1:].contiguous().view(-1) \n",
    "        loss = loss_fn(output,target.to(device))\n",
    "        losses.append(loss.item())\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        optimizer.step() # step optimzier \n",
    "        n_iter+=1 # count number of trained sentences\n",
    "        if n_iter % 100 == 0: # print loss info for every 100 iteration steps\n",
    "            print ('\\n [{}] current_iter_loss= {:05.3f}'.format(n_iter,loss))\n",
    "  \n",
    "    print ('\\n Epoch({}/{}) avg_loss= {:05.3f}'.format(epoch+1,n_epochs,np.mean(losses)))\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2word(idx, i2w, pad_idx):\n",
    "    \"\"\"\n",
    "    index로 이루어진 문장을 받아,\n",
    "    word 문장으로 전환\n",
    "    \"\"\"\n",
    "\n",
    "    sent_str = [str()]*len(idx)\n",
    "\n",
    "    for i, sent in enumerate(idx):\n",
    "\n",
    "        for word_id in sent:\n",
    "\n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            sent_str[i] += i2w[int(word_id)] + \" \"\n",
    "\n",
    "        sent_str[i] = sent_str[i].strip()\n",
    "\n",
    "\n",
    "    return sent_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1VmQoyICZAS"
   },
   "outputs": [],
   "source": [
    "def lm_eval(model, loader):\n",
    "    losses = []\n",
    "\n",
    "    model.eval() # valid mode\n",
    "    n_iter = 0\n",
    " \n",
    "    for batch in loader:\n",
    "        input, input_lengths, _ = batch\n",
    "        output = model(input[:,:-1].contiguous().to(device),(input_lengths-1).to(device)) # x vocab_size\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "        target = input[:,1:].contiguous().view(-1)\n",
    "        loss = loss_fn(output,target.to(device))\n",
    "        losses.append(loss.item())\n",
    "    prediction = torch.topk(output, 1)[1].squeeze(1).view(input.shape[0],-1)\n",
    "    print(\"\\n[Output sentence:] \")\n",
    "    print(idx2word(prediction, id2word, 0) + '\\n')\n",
    "    print(\"[Target sentence:] \")\n",
    "    print(idx2word(input[:,1:], id2word, 0))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQX4dcRbCZAU"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-955c326db4a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlm_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlm_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'\\n              avg_valid loss= {:05.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-c58467504ecf>\u001b[0m in \u001b[0;36mlm_train\u001b[1;34m(model, optimizer, train_loader, epoch, n_epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LM\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-103-900f42da1ac0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, lengths)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0minput_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# embedding layer  (B,L) -> (B,L ,hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpacked_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pack sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpadded_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# packed sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\LM\\lib\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first)\u001b[0m\n\u001b[0;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m    147\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LM(config, vocab_size=len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = lm_train(model.to(device),optimizer, train_data_loader, epoch,config[\"epochs\"]) # train\n",
    "    valid_loss = lm_eval(model.to(device), valid_data_loader) # valid\n",
    "    print ('\\n              avg_valid loss= {:05.3f}'.format(np.mean(valid_loss)))\n",
    "    test_loss = lm_eval(model.to(device), test_data_loader) # test\n",
    "    print ('\\n              avg_test loss= {:05.3f}'.format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NE0RiDBCZAV"
   },
   "source": [
    "### 더 큰 hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auUuCfjxCZAW"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 128,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 128,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZjDoJTdyCZAX"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LM(config, vocab_size=len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = lm_train(model.to(device),optimizer, train_data_loader, epoch,config[\"epochs\"]) # train\n",
    "    valid_loss = lm_eval(model.to(device), valid_data_loader) # valid\n",
    "    print ('\\n              avg_valid loss= {:05.3f}'.format(np.mean(valid_loss)))\n",
    "    test_loss = lm_eval(model.to(device), test_data_loader) # test\n",
    "    print ('\\n              avg_test loss= {:05.3f}'.format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJRvIJy8CZAY"
   },
   "source": [
    "### GRU로 변경하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kg8raisCZAY"
   },
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(LM, self).__init__()\n",
    "        self.vocab_size = vocab_size # get vocabulary size \n",
    "        self.embedding = nn.Embedding(self.vocab_size, config[\"embedding_size\"])  # initialize embedding layer \n",
    "        self.rnn = nn.GRU(input_size=config[\"embedding_size\"], hidden_size=config[\"hidden_size\"], \n",
    "                          dropout=config[\"dropout\"], batch_first=True) # defined rnn layer \n",
    "        self.outputs2vocab = nn.Linear(config[\"hidden_size\"], self.vocab_size) # linear layer to predict the sentiment\n",
    "    def forward(self,input, lengths):\n",
    "        batch_size = input.size(0) # get batch_size\n",
    "        input_embedding = self.embedding(input) # embedding layer  (B,L) -> (B,L ,hidden)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, lengths, batch_first=True) # pack sequence \n",
    "        outputs, _ = self.rnn(packed_input)   # rnn \n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True, padding_value=0.)[0] # packed sequence \n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        outputs = self.outputs2vocab(padded_outputs) \n",
    "        logits = outputs.contiguous().view(-1, self.vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEuV0t45CZAa"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 128,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 128,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbD9AdPMCZAb"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LM(config, vocab_size=len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = lm_train(model.to(device),optimizer, train_data_loader, epoch,config[\"epochs\"]) # train\n",
    "    valid_loss = lm_eval(model.to(device), valid_data_loader) # valid\n",
    "    print ('\\n              avg_valid loss= {:05.3f}'.format(np.mean(valid_loss)))\n",
    "    test_loss = lm_eval(model.to(device), test_data_loader) # test\n",
    "    print ('\\n              avg_test loss= {:05.3f}'.format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Utsua4KCCZAd"
   },
   "source": [
    "# Chatper 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y05zyrnMCZAd"
   },
   "source": [
    "# Sentiment Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCayS8BnCZAd"
   },
   "source": [
    "![대체 텍스트](./figures/many_many.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gix2y-OLCZAd"
   },
   "source": [
    "![대체 텍스트](./figures/sentence_clas.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVRs6V0JCZAe"
   },
   "outputs": [],
   "source": [
    "class Sentiment_Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, path , word2id):\n",
    "        self.seqs = open(path).readlines()\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and sentiment).\"\"\"\n",
    "        seqs = self.seqs[index]\n",
    "        seqs, label = self.process(seqs, self.word2id)\n",
    "        return seqs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "                    \n",
    "    def process(self, seq, word2id):\n",
    "        label = 0 # default label \"pos\"\n",
    "        sequence = []\n",
    "        sequence.append(word2id[\"<s>\"])\n",
    "        words = seq.strip().split(' ')\n",
    "        for i in range(0, len(words)-1):\n",
    "            current_word = words[i]\n",
    "            if \"negative\" == words[len(words)-1]: #if label is \"neg\", then 1\n",
    "                label = 1\n",
    "            if current_word in word2id:\n",
    "                sequence.append(word2id[current_word])\n",
    "            else:\n",
    "                sequence.append(3) # replace by <unk> token\n",
    "        sequence.append(word2id[\"</s>\"])\n",
    "        sequence = torch.Tensor(sequence)\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puWHJTZOCZAf"
   },
   "outputs": [],
   "source": [
    "class Sentiment_Classification(nn.Module):\n",
    "    \"\"\"\n",
    "    IMDb 영화 리뷰 감성분류 모델\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            config     - hyperparameters\n",
    "            vocab_size - vocab_size\n",
    "        return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Sentiment_Classification, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, config[\"embedding_size\"])\n",
    "        self.rnn = nn.RNN(input_size=config[\"embedding_size\"], hidden_size=config[\"hidden_size\"], \n",
    "                          num_layers=config[\"num_layers\"],dropout=config[\"dropout\"],bidirectional = config['bidirectional'] , batch_first=True)\n",
    "        self.bidirectional = config['bidirectional']\n",
    "        self.num_layers = config['num_layers']\n",
    "        if self.bidirectional:\n",
    "            self.hidden_size = 2*config['hidden_size']\n",
    "        else:\n",
    "            self.hidden_size = config['hidden_size']\n",
    "        self.outputs = nn.Sequential(\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, 1),\n",
    "                            nn.Sigmoid()\n",
    "                        )\n",
    "    def forward(self,input, lengths):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            input   - setences\n",
    "            lengths - setence lengths\n",
    "        return:\n",
    "            outputs - positive or negative\n",
    "        \"\"\"\n",
    "        batch_size = input.size(0)\n",
    "        input_embedding = self.embedding(input)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, lengths, batch_first=True)\n",
    "        _, hidden = self.rnn(packed_input)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            batch_size = hidden.size()[1]\n",
    "            hidden = hidden.view(self.num_layers,2,batch_size,-1)\n",
    "            hidden = hidden.permute(2,0,1,3).contiguous().view(batch_size,self.num_layers,-1)\n",
    "        else:\n",
    "            hidden = hidden.permute(1,0,2)\n",
    "            \n",
    "        hidden = hidden[:,-1,:]\n",
    "        outputs = self.outputs(hidden)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EI1PUEyFCZAg"
   },
   "source": [
    "![대체 텍스트](./figures/BCE1.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9edpKCUCZAg"
   },
   "source": [
    "![대체 텍스트](./figures/BCE2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRFIMOIOCZAh"
   },
   "outputs": [],
   "source": [
    "def sentiment_trainer(model, optimizer, loaders, epoch,n_epochs):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model     - sentiment model\n",
    "        optimizer - adam\n",
    "        loaders   - valid, train loaders\n",
    "    return:\n",
    "        model, optimizer\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    total_accuracy = []\n",
    "    val_losses = []\n",
    "    val_total_accuracy = []\n",
    "    n_iter = 0\n",
    "    for split in loaders.keys():\n",
    "        if split == \"train\":\n",
    "            for batch in loaders[split]:\n",
    "                model.train() # train mode\n",
    "                input, input_lengths, target_label = batch\n",
    "                predict_label = model(input.to(device),input_lengths.to(device))\n",
    "                loss_fn = nn.BCELoss().to(device)\n",
    "                loss = loss_fn(predict_label,target_label.to(device))\n",
    "                losses.append(loss.item())\n",
    "                # Calculate accuracy\n",
    "                x_acc = predict_label.round().cpu().detach().numpy()\n",
    "                y_acc = target_label.cpu().detach().numpy()\n",
    "                accuracy = accuracy_score(x_acc,y_acc)\n",
    "                total_accuracy.append(accuracy)\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                n_iter+=1 # count number of trained sentences\n",
    "                if n_iter % 100 == 0: # print loss only if it's training stage\n",
    "                    print ('\\n [{}] current_iter_loss= {:05.3f} acc= {:05.3f}'.format(n_iter,loss,accuracy))\n",
    "                \n",
    "        elif split == \"valid\":\n",
    "            model.eval() # eval mode\n",
    "            for batch in loaders[split]:\n",
    "                input, input_lengths, target_label = batch\n",
    "                predict_label = model(input.to(device),input_lengths.to(device))\n",
    "                loss_fn = nn.BCELoss().to(device)\n",
    "                loss = loss_fn(predict_label,target_label.to(device))\n",
    "                val_losses.append(loss.item())\n",
    "                # Calculate accuracy\n",
    "                x_acc = predict_label.round().cpu().detach().numpy()\n",
    "                y_acc = target_label.cpu().detach().numpy()\n",
    "                accuracy = accuracy_score(x_acc,y_acc)\n",
    "                val_total_accuracy.append(accuracy)\n",
    "  \n",
    "    print ('\\n Epoch({}/{}) avg train_loss= {:05.3f} train_acc= {:05.3f} val_loss= {:05.3f} val_acc= {:05.3f}'\n",
    "           .format(\n",
    "               epoch+1,n_epochs,np.mean(losses), np.mean(total_accuracy), \n",
    "                   np.mean(val_losses), np.mean(val_total_accuracy)\n",
    "              )\n",
    "          )\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdAsGWA9CZAj"
   },
   "outputs": [],
   "source": [
    "def idx2word(idx, i2w, pad_idx):\n",
    "    \"\"\"\n",
    "    index로 이루어진 문장을 받아,\n",
    "    word 문장으로 전환\n",
    "    \"\"\"\n",
    "\n",
    "    sent_str = [str()]*len(idx)\n",
    "\n",
    "    for i, sent in enumerate(idx):\n",
    "\n",
    "        for word_id in sent:\n",
    "\n",
    "            if word_id == pad_idx:\n",
    "                break\n",
    "            sent_str[i] += i2w[int(word_id)] + \" \"\n",
    "\n",
    "        sent_str[i] = sent_str[i].strip()\n",
    "\n",
    "\n",
    "    return sent_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrlm2rdoCZAk"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, epoch,n_epochs, id2word):\n",
    "    \"\"\"\n",
    "    각 epoch 마다 model에 대한 test및, prediction결과 추출\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    total_accuracy = []\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        input, input_lengths, target_label = batch\n",
    "        predict_label = model(input.to(device),input_lengths.to(device))\n",
    "        loss_fn = nn.BCELoss().to(device)\n",
    "        loss = loss_fn(predict_label,target_label.to(device))\n",
    "        losses.append(loss.item())\n",
    "        # Calculate accuracy\n",
    "        x_acc = predict_label.round().cpu().detach().numpy()\n",
    "        y_acc = target_label.cpu().detach().numpy()\n",
    "        accuracy = accuracy_score(x_acc,y_acc)\n",
    "        total_accuracy.append(accuracy)\n",
    "\n",
    "    print ('\\n Epoch({}/{}) avg test_loss= {:05.3f} test_acc= {:05.3f}'\n",
    "        .format(epoch+1,n_epochs,np.mean(losses), np.mean(total_accuracy))\n",
    "    )\n",
    "    \n",
    "    # test inference\n",
    "    print (idx2word(input, id2word, 0))\n",
    "    predict_label = model(input.to(device),input_lengths.to(device))\n",
    "    ans = {0: 'positive', 1:'negative'}\n",
    "    print('\\n Pred: {}, Ans: {}'\n",
    "          .format(ans[predict_label[0].round().item()], ans[target_label[0].item()])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f3apmvcCZAn"
   },
   "outputs": [],
   "source": [
    "seqs = open(\"./Data/lm_data/new_IMDb/train.tok\").readlines()\n",
    "vocab, word2id, id2word = build_dict(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OF2Eje0mCZAo"
   },
   "outputs": [],
   "source": [
    "# Building vocab or loading existing vocab\n",
    "path = \"./Data/sentimen_id2word.pkl\"\n",
    "if os.path.isfile(path):\n",
    "    with open(\"./Data/sentimen_id2word.pkl\", \"rb\") as f:\n",
    "        id2word = pickle.load(f)\n",
    "    with open(\"./Data/sentimen_word2id.pkl\", \"rb\") as f:\n",
    "        word2id = pickle.load(f)\n",
    "    with open(\"./Data/sentiment_vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "else: # file does not exist\n",
    "    vocab, word2id, id2word = build_dict(seqs)\n",
    "    pickle.dump(vocab, open(\"./Data/sentiment_vocab.pkl\", \"wb\" ))\n",
    "    pickle.dump(word2id, open(\"./Data/sentiment_word2id.pkl\", \"wb\" ))\n",
    "    pickle.dump(id2word, open(\"./Data/sentiment_id2word.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0iRm39HCZAs"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 64,\n",
    "  \"bidirectional\": True,\n",
    "  \"num_layers\": 3,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 64,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7tBAGarCZAt",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# constructing train dataset\n",
    "train_dataset = Sentiment_Dataset(\"./Data/lm_data/new_IMDb/train.tok\", word2id)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "# constructing valid dataset\n",
    "valid_dataset = Sentiment_Dataset(\"./Data/lm_data/new_IMDb/valid.tok\", word2id)\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "# constructing test dataset\n",
    "test_dataset = Sentiment_Dataset(\"./Data/lm_data/new_IMDb/test.tok\", word2id)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "loaders = {\"train\": train_data_loader, \"valid\": valid_data_loader}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Init sentiment model\n",
    "model2 = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model2 = model2.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = sentiment_trainer(model2,optimizer, loaders, epoch,config[\"epochs\"])\n",
    "    evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17-IfdwWCZAu"
   },
   "source": [
    "## No Bidirectional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gv2Md4qmCZAv"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 64,\n",
    "  \"bidirectional\": True,\n",
    "  \"num_layers\": 2,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 64,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdvYvw9kCZAw"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model2 = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model2 = model2.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = sentiment_trainer(model2,optimizer, loaders, epoch,config[\"epochs\"])\n",
    "    evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ck_Fu9ICZAx"
   },
   "source": [
    "### GRU Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eR50L1cCZAy"
   },
   "outputs": [],
   "source": [
    "class Sentiment_Classification(nn.Module):\n",
    "\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(Sentiment_Classification, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, config[\"embedding_size\"])\n",
    "        self.rnn = nn.GRU(input_size=config[\"embedding_size\"], hidden_size=config[\"hidden_size\"], \n",
    "                          num_layers=config[\"num_layers\"],dropout=config[\"dropout\"],bidirectional = config['bidirectional'] , batch_first=True)\n",
    "        self.bidirectional = config['bidirectional']\n",
    "        self.num_layers = config['num_layers']\n",
    "        if self.bidirectional:\n",
    "            self.hidden_size = 2*config['hidden_size']\n",
    "        else:\n",
    "            self.hidden_size = config['hidden_size']\n",
    "        self.outputs = nn.Sequential(\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                            nn.LeakyReLU(0.2),\n",
    "                            nn.Linear(self.hidden_size, 1),\n",
    "                            nn.Sigmoid()\n",
    "                        )\n",
    "    def forward(self,input, lengths):\n",
    "        batch_size = input.size(0)\n",
    "        input_embedding = self.embedding(input) \n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, lengths, batch_first=True)\n",
    "        _, hidden = self.rnn(packed_input)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            batch_size = hidden.size()[1]\n",
    "            hidden = hidden.view(self.num_layers,2,batch_size,-1)\n",
    "            hidden = hidden.permute(2,0,1,3).contiguous().view(batch_size,self.num_layers,-1)\n",
    "        else:\n",
    "            hidden = hidden.permute(1,0,2)\n",
    "            \n",
    "        hidden = hidden[:,-1,:]\n",
    "        outputs = self.outputs(hidden)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0lyd4kCCZAz"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 64,\n",
    "  \"bidirectional\": True,\n",
    "  \"num_layers\": 2,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 64,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTtlWvh3CZA0"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model2 = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model2 = model2.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = sentiment_trainer(model2,optimizer, loaders, epoch,config[\"epochs\"])\n",
    "    evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvzW2r5ECZA2"
   },
   "source": [
    "## Let's Do it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oCFfysBCZA2"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"epochs\": 5,\n",
    "  \"hidden_size\": 64,\n",
    "  \"bidirectional\": True,\n",
    "  \"num_layers\": 2,\n",
    "  \"dropout\": 0.2,\n",
    "  \"batch_size\": 32,\n",
    "  \"embedding_size\": 64,\n",
    "  \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWPCdklzCZA5"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model2 = Sentiment_Classification(config, vocab_size=len(vocab))\n",
    "model2 = model2.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=config.get(\"learning_rate\", .001))\n",
    "\n",
    "# Train model\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    model,optimizer  = sentiment_trainer(model2,optimizer, loaders, epoch,config[\"epochs\"])\n",
    "    evaluate(model, test_data_loader, epoch,config[\"epochs\"], id2word)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "(ANS)Word_Embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
